{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    " \n",
    "figDir = \"Figures/\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Data preperation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def split(): print(\"\\n____________________________________________________________________________________\\n\")\n",
    "\n",
    "# Tüm featureler için korelasyon matrisi\n",
    "# Correlation matrix for all features\n",
    "def plotCorrelationMatrix(df, graphWidth):  \n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for xd', fontsize=40)\n",
    "    plt.show()\n",
    "#Boxplot, gruplama ve korelasyonların hepsini analiz eden all-in-one fonksiyon\n",
    "# ALL-IN-ONE function for data analysis\n",
    "    plt.savefig(figDir+'CorrelationMatrix.png')\n",
    "def intro(df,graph=True,splitPlots=True,EraseNullColumns=False,printCorrelations=True,corrThreshold=0.5):\n",
    "    \n",
    "    dataframe=df.copy()\n",
    "    \n",
    "    if(EraseNullColumns==True):  dataframe.dropna(axis=1,inplace=True)\n",
    "\n",
    "    split()\n",
    "    print(df)\n",
    "    split()\n",
    "    print(dataframe.head(5))\n",
    "    split()\n",
    "    \n",
    "    print(dataframe.info())\n",
    "    split()\n",
    "    \n",
    "    print(dataframe.describe())\n",
    "    split()\n",
    "    \n",
    "#-------------------------------BOXPLOTFEATURES-----------------------------      \n",
    "    \n",
    "    \n",
    "    if(graph):\n",
    "\n",
    "        if(splitPlots==True):\n",
    "            print(\"                         ___BOXPLOTFETURES\")\n",
    "\n",
    "            for column in dataframe.columns:\n",
    "                if(dataframe[column].dtype==np.int or dataframe[column].dtype==np.float):\n",
    "                    plt.figure()\n",
    "                    dataframe.boxplot([column])\n",
    "                    plt.savefig(figDir+'{}.png'.format(column))\n",
    "                    \n",
    "        else:\n",
    "            dataframe.boxplot()\n",
    "            \n",
    "    #If unique values of columns is under 10, print unique values with considered column\n",
    "\n",
    "\n",
    "#-------------------------------GROUPBY-----------------------------        \n",
    "\n",
    "    print(\"                         _____GROUPBY____\")\n",
    "\n",
    "    for column in dataframe.columns:    \n",
    "        unique_values=dataframe[column].unique()\n",
    "        if(unique_values.size<=10):\n",
    "            print(column,\": \",unique_values)\n",
    "            print(\"\\nGrouped By: \",column,\"\\n\\n\",dataframe.groupby(column).mean())\n",
    "            split()\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        \n",
    "#-------------------------------CORRELATIONS-----------------------------        \n",
    "    if(printCorrelations==True):\n",
    "        print(\"                         ____CORRELATIONS____\")\n",
    "        corrByValues= dataframe.corr().copy()\n",
    "        flag = False\n",
    "        corr_matrix=abs(corrByValues>=corrThreshold)\n",
    "        columns= corr_matrix.columns\n",
    "        for i in range(columns.size):\n",
    "            for j in range(i,columns.size):\n",
    "                iIndex=columns[i]\n",
    "                jIndex=columns[j] \n",
    "                if (i!=j and corr_matrix[iIndex][jIndex]==True and (len(df[iIndex].unique())!=1 and len(df[jIndex].unique())!=1 )):\n",
    "                    sign = \"Positive\"\n",
    "                    if(corrByValues[iIndex][jIndex]<0): sign=\"Negative\"\n",
    "                    split()\n",
    "                    flag = True\n",
    "                    print(iIndex.upper(), \" has a \" ,sign,\" correlation with \",jIndex.upper(),\": {} \\n\".format(corrByValues[iIndex][jIndex]))\n",
    "        \n",
    "        plt.show()\n",
    "        plotCorrelationMatrix(df,30)       \n",
    "        \n",
    "        split()\n",
    "        if(not flag):\n",
    "            print(\"No Correlation Found\") \n",
    "    return dataframe\n",
    "\n",
    "# KDE dağılımı ile featureları plotlar\n",
    "# KDE plot function that plots all features in given dataframe\n",
    "def plotCols(df,time):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if(df[col].dtype==np.int or df[col].dtype==np.float):\n",
    "            if(len(df[col].unique())>1):\n",
    "                fig = df.plot(x=time,y=col,kind=\"kde\", title = \"{}-{} KDE\".format(time,col))    \n",
    "                fig.get_figure().savefig(figDir+\"{}-kde.png\".format(time+\"-\"+col))\n",
    "                plt.show()\n",
    "            plt.plot(df[time],df[col]) \n",
    "            plt.title(\"{}-{}\".format(time,col))\n",
    "            plt.show()\n",
    "            plt.savefig(figDir+'{}.png'.format(time+\"-\"+col))\n",
    "        \n",
    "#Verilen feature'ları scatter ile Y'ye göre karşılaştırır.      \n",
    "#Function for comparing given features in dataframe with Y feature\n",
    "def XCorrWithY(df, X, Y):\n",
    "    for col in  X:\n",
    "        print(col,\"-\",Y)\n",
    "        plt.scatter(df[col],df[Y]) \n",
    "        plt.title(\"{}-{}\".format(col,Y))\n",
    "        plt.show()    \n",
    "#Dataframeyi normalize eder. (Preprocessing)\n",
    "#Normalizes dataframe\n",
    "def normalizedf(df,offset=0):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() \n",
    "    new = df.copy()\n",
    "    cols = df.columns[offset:]\n",
    "    new[cols] = (min_max_scaler.fit_transform(new[cols]) )  \n",
    "    return new    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all machines' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"unit_num\",\"time_in_cycles\" ]\n",
    "for i in range(3):\n",
    "    colnames.append(\"operational_setting{}\".format(i+1))\n",
    "for i in range(21):\n",
    "    colnames.append(\"s{}\".format(i+1))\n",
    "    \n",
    "def getData(prefix,num,names=colnames):\n",
    "        return pd.read_csv('{}_FD00{}.txt'.format(prefix,num), delim_whitespace=True, header = None, names= names )  \n",
    "        \n",
    "traindfs = []\n",
    "testdfs = [] \n",
    "testYs = []\n",
    "\n",
    "for i in range(4):\n",
    "    traindfs.append(getData(\"train\",i+1) ) \n",
    "      \n",
    "for i in range(4): \n",
    "    testdfs.append(getData(\"test\",i+1) )       \n",
    "     \n",
    "for i in range(4):\n",
    "    testYs.append(getData(\"RUL\",i+1,names=[\"Y\"]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose type of machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "traindf = traindfs[index]\n",
    "testdf  = testdfs[index]\n",
    "testY   = testYs[index]  \n",
    "#           R^2   MAE     RMSE \n",
    "# Index 0  0.74  17.27   21.27 : LSTM\n",
    "# Index 0  0.79  15.09   18.8 : LSTM, CROP 50\n",
    "# Index 0  0.72  17.27   27.09 : STACK MLP+LASSO -> MLP LOOK_BACK: 31\n",
    "# Index 0  0.73  16.31   21.65 : LSTM, PADDING 50\n",
    "# Index 1  0.61  25.66   33.64 : LGMRegressor           LOOK_BACK: 21\n",
    "# Index 2  0.51  20.49   29.01 : STACK MLP+LASSO -> MLP LOOK_BACK: 35\n",
    "# Index 2  0.62  19.57   25.66 : STACK MLP+LASSO -> MLP LOOK_BACK: 50 PADDING\n",
    "# Index 3 -> look back = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro(traindf)\n",
    "plotCols(traindf,\"time_in_cycles\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Preprocessing functions on data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sabit kalan feature'ları temizle\n",
    "def removeSame(df,threshold=1):\n",
    "    new = df.copy()\n",
    "    willremove = [] \n",
    "    for col in new.columns:\n",
    "        if(len(new[col].unique())<=threshold):\n",
    "            del new[col] \n",
    "            willremove.append(col)\n",
    "    return new,willremove\n",
    "\n",
    "#Korelasyonu fazla olan feature'leri temizle\n",
    "def get_train_columns(df,CorrThreshold):\n",
    "    corrByValues= df.corr().copy()\n",
    "    corrMat = abs(corrByValues)>=CorrThreshold\n",
    "    print(corrMat)\n",
    "    columnList = df.columns.to_list()\n",
    "    length = len(columnList)\n",
    "    features = columnList\n",
    "    for i in range(length):\n",
    "        for j in range(i+1,length):\n",
    "            if(corrMat.iloc[i,j]==True and df.columns[j] in features):\n",
    "                features.remove(df.columns[j])\n",
    "    return features  \n",
    "\n",
    "#Time-Series bir şekilde train ve test verilerini ayarla\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset.iloc[i:(i+look_back), :-1]\n",
    "\t\tdataX.append(a.to_numpy())\n",
    "\t\tdataY.append(dataset.iloc[i + look_back, -1]) \n",
    "\treturn np.array(dataX), np.array(dataY)  \n",
    "trainX = traindf\n",
    "testX = testdf\n",
    "#Her devirde Sabit kalan değerleri çıkar \n",
    "trainX,deletedCols = removeSame(traindf.iloc[:,:],threshold=1)\n",
    "testX = testdf.drop(columns=deletedCols)   \n",
    "\n",
    "#Korelasyonu başka feature'lardan belirli seviteen fazla olan fetureları çıkar \n",
    "input_features = get_train_columns(trainX,0.75) \n",
    "trainX, testX = trainX[input_features], testX[input_features]\n",
    "#Her unit için çalıştığı en fazla devir\n",
    "trainYs = traindf.groupby([\"unit_num\"]).time_in_cycles.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP = True\n",
    "PADDING = True \n",
    "#Prepare train for all units without intercepting with each other     \n",
    "look_back = 50 #min(trainYs.min(),testX.groupby([\"unit_num\"]).time_in_cycles.max().min())\n",
    "assert look_back<=min(trainYs.min(),testX.groupby([\"unit_num\"]).time_in_cycles.max().min()) or CROP\n",
    "unitnums = trainX.unit_num.unique() \n",
    "def Padding(tX):\n",
    "    unitnums = tX.unit_num.unique()\n",
    "    dfPerUnit = []\n",
    "    for i in unitnums:\n",
    "        maxCycle = int(tX.loc[tX.unit_num==i].time_in_cycles.max())\n",
    "        tmp = tX.loc[tX.unit_num==i]\n",
    "        if(maxCycle<look_back):\n",
    "            for j in range(look_back-maxCycle):\n",
    "                tmp = tmp.append(tmp.iloc[-1] , ignore_index= True)     \n",
    "                tmp.iloc[-1].time_in_cycles = maxCycle + j + 1 \n",
    "        dfPerUnit.append(tmp)\n",
    "    return pd.concat(dfPerUnit)    \n",
    "    \n",
    "X = trainX.copy()\n",
    "Y = trainYs.copy()\n",
    "tX = testX.copy()\n",
    "tY = testY.copy()\n",
    "if(PADDING):\n",
    "    tX = Padding(tX)\n",
    "    X = Padding(X) \n",
    "\n",
    "X[\"time\"] = X[\"time_in_cycles\"]\n",
    "tX[\"time\"] = tX[\"time_in_cycles\"]\n",
    "\n",
    "X, tX = normalizedf(X,offset=2).fillna(0), normalizedf(tX,offset=2).fillna(0) \n",
    "\n",
    "\n",
    "X[\"Y\"]=X.time_in_cycles\n",
    "for i in range(len(unitnums)): \n",
    "    X[\"Y\"].loc[X.unit_num==unitnums[i]] -= Y.iloc[i] \n",
    "    X[\"Y\"]= abs(X[\"Y\"])  \n",
    "    \n",
    "unitnums = trainX.unit_num.unique()    \n",
    " \n",
    "x, y = create_dataset(X.loc[X.unit_num==unitnums[0]].iloc[:,2:],look_back)\n",
    "arrX = x\n",
    "arrY = y\n",
    "for i in range(1,len(unitnums)): \n",
    "    machineData = X.loc[X.unit_num==unitnums[i]]\n",
    "    if(CROP and machineData.shape[0]>look_back):\n",
    "        x, y = create_dataset(machineData.iloc[:,2:],look_back)\n",
    "        arrX = np.vstack((arrX,x))\n",
    "        arrY = np.append(arrY,y)\n",
    "print(tX.shape,X.shape)\n",
    "\n",
    "#Prepare test for all units without intercepting with each other    \n",
    "    \n",
    "testarrX = [tX[tX['unit_num']==id].values[-look_back:] for id in unitnums if (CROP and tX[tX['unit_num']==id].shape[0]>=look_back)]\n",
    "print(tX.shape,X.shape)\n",
    "\n",
    "if(not testarrX[-1].shape[0]):\n",
    "    testarrX = testarrX[:-1]\n",
    "testarrX = np.asarray(testarrX ).astype(np.float32)\n",
    "\n",
    "tX = testarrX[:,:,2:] \n",
    "remainingIdsAfterCrop = testarrX[:,0,0].astype(int)\n",
    "X = arrX\n",
    "Y = arrY \n",
    "#edit tY for remainingIds after Crop. (-1 for mapping ids to indexes)\n",
    "tY = testY.iloc[remainingIdsAfterCrop-1].Y\n",
    "featureCount = X.shape[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape: \",X.shape,\" \", Y.shape,\"\\nTest shape:\",tX.shape,\" \",tY.shape)\n",
    "np.random.seed(42)\n",
    "assert(X.shape[1:]==tX.shape[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long short-term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, LeakyReLU  \n",
    "\n",
    "from keras.optimizers import Adam, SGD, Adamax,RMSprop  \n",
    "import tensorflow as tf \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(look_back, featureCount),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(units=1,activation=\"linear\")) \n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'  ])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(X, Y, epochs=100, batch_size=200 ,  validation_data=(tX,tY), verbose=2,use_multiprocessing=True,\n",
    "          callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
    "                       ModelCheckpoint(\"modelcropped.h5\",monitor='val_loss', save_best_only=True, mode='min', verbose=1)]\n",
    "          )\n",
    "\n",
    "#Plot History\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"LSTM Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Preprocessing and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score \n",
    "def Reshape3D(X):\n",
    "    return np.reshape(X,(X.shape[0], featureCount*look_back))  \n",
    "def testModel(model,MtX,tY): \n",
    "    testPredict = model.predict(MtX)\n",
    "    testPredict = np.reshape(testPredict,testPredict.shape[0]) \n",
    "    tY = tY.astype(\"float\")\n",
    "    testPredict = testPredict.astype(\"float\")  \n",
    "    testScore = (mean_absolute_error(tY, testPredict))\n",
    "    root_mse = math.sqrt(mean_squared_error(tY,testPredict))\n",
    "    r2score = r2_score(tY,testPredict)\n",
    "    print(str(model)+'\\nTest Score: %.2f MAE' % (testScore))\n",
    "    print('Test Score: %.2f RMSE' % (root_mse))\n",
    "    print('Test Score: %.2f r2' % (r2score)) \n",
    "    plt.plot(testPredict)\n",
    "    plt.plot(tY)\n",
    "    plt.title(str(model))\n",
    "    plt.show()\n",
    "    return testScore,r2score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"modelCROP.h5\",compile=True)\n",
    "testModel(model,tX,tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Training and Test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso,Ridge, BayesianRidge \n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn import tree \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "models = [LGBMRegressor()]#LinearRegression(), xgb.XGBRegressor(),Lasso(),MLPRegressor(max_iter=500),LGBMRegressor(),CatBoostRegressor( ),Ridge(),BayesianRidge(),tree.DecisionTreeRegressor(),svm.SVR(),GradientBoostingRegressor()] \n",
    "MtX = Reshape3D(tX)  \n",
    "MX = Reshape3D(X)\n",
    "testScores = []\n",
    "r2scores = []\n",
    "for MLmodel in models:\n",
    "    MLmodel = MLmodel.fit(MX,Y)\n",
    "    testScore,r2score = testModel(MLmodel,MtX,tY)\n",
    "    testScores.append(testScore)\n",
    "    r2scores.append(r2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensembling Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacking ensemble\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "# define dataset\n",
    "# lasso + mlp -> mlp = 0.66 r^2 18.77 MAE,  eğer 30 devire bakarssa 0.68, cv=2, iter = 200 \n",
    "# lasso + mlp -> mlp = 0.7 r^2, 17.57 MAE, 22.76 RMSE, 31 look_back, 250 iterasyon, 0.75 feature threshold, removesame , cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
    "# lasso + mlp -> mlp = 0.71 r^2, 17.03 MAE, 22.50 RMSE, 31 look_back, 300 iterasyon, 0.75 feature threshold, removesame , cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
    "# lasso + mlp -> mlp = 0.72 r^2, 17.27 MAE, 22.09 RMSE, 31 look_back, 300 iterasyon, 0.75 feature threshold, removesame th=2, cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
    "\n",
    "# Lasso + mlp -> svm = 0.68 r^2, 17.61 MAE, 30 devir cv=2\n",
    "# 31 devir, Lasso + mlp -> mlp = 0.67 r^2, 18.42 MAE\n",
    "# define the base models\n",
    "def ScatterPredictions(models,X,Y):\n",
    "    axis = np.arange(X.shape[0]) \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.scatter(axis, Y, s=10,  label=\"REAL Y\", c='#FF4500')   \n",
    "    for model in models: \n",
    "        ax1.scatter(axis, model.predict(X), s=10,  label=str(model)[:10]) \n",
    "    \n",
    "    plt.title(\"Comparison of model predictions\")\n",
    "    plt.show()  \n",
    "    \n",
    "MtX = Reshape3D(tX)  \n",
    "MX = Reshape3D(X)\n",
    "print(MX.shape,Y.shape,MtX.shape,tY.shape)\n",
    "max_iter = 300\n",
    "\"\"\"\n",
    "models = [Lasso().fit(MX,Y), MLPRegressor(max_iter=max_iter).fit(MX,Y)]\n",
    "ScatterPredictions(models,MX,Y) \"\"\" \n",
    "level0 = list()\n",
    "level0.append(('lasso', Lasso())) \n",
    "level0.append(('mlp', MLPRegressor(max_iter=max_iter)))    \n",
    "# define meta learner model\n",
    "level1 = MLPRegressor(max_iter=max_iter )\n",
    "# define the stacking ensemble\n",
    "model = StackingRegressor(estimators=level0, final_estimator=level1, cv=2)\n",
    "# fit the model on all available data\n",
    "model =  model.fit(MX, Y)\n",
    "# make a prediction for one example\n",
    "testModel(model,MtX,tY)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "#%% ANALYSIS ABOUT REGRESSION\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X, Y, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=trainX.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(model, 'MODELNAME.pkl')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
